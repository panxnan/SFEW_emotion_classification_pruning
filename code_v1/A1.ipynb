{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "executionInfo": {
     "elapsed": 1764,
     "status": "ok",
     "timestamp": 1618922998542,
     "user": {
      "displayName": "潘xn",
      "photoUrl": "",
      "userId": "15665619575579015109"
     },
     "user_tz": -480
    },
    "id": "t-Gy0kW6uKh4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define dataloader, Neural network, and other until functions\n",
    "- csv2dataset: load data from xlsx file\n",
    "- Multip_layer: three layer network, the hidden layer size could be modified\n",
    "- get_overall_accuray, get_class_wise_accuracy: function to get accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 880,
     "status": "ok",
     "timestamp": 1618923000645,
     "user": {
      "displayName": "潘xn",
      "photoUrl": "",
      "userId": "15665619575579015109"
     },
     "user_tz": -480
    },
    "id": "JUxh0O5PuKiD"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fist step load data from xlsx\n",
    "\"\"\"\n",
    "def csv2dataset(path='data/SFEW.xlsx'):\n",
    "    exl = pd.read_excel(path)\n",
    "    header = exl.columns\n",
    "    idx_header = range(2,12)\n",
    "\n",
    "    pca = exl[header[idx_header]].values\n",
    "    pca = np.delete(pca, (205), axis=0)  # remove row 205 which contains nan value\n",
    "    target = exl[header[1]].values\n",
    "    target = np.delete(target, (205), axis=0)\n",
    "\n",
    "    input_tensor = torch.tensor(pca, dtype=torch.float32)\n",
    "    # transform target [1, 7] -> [0, 6]\n",
    "    target_tensor = torch.tensor(target, dtype=torch.long) - 1\n",
    "\n",
    "    # apply normalization\n",
    "    input_mean = torch.mean(input_tensor, dim=0)\n",
    "    input_std = torch.std(input_tensor, dim=0)\n",
    "    input_tensor = (input_tensor - input_mean) / input_std\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(input_tensor, target_tensor)\n",
    "    print('load data set: %d samples' % len(dataset))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "executionInfo": {
     "elapsed": 1255,
     "status": "ok",
     "timestamp": 1618923008243,
     "user": {
      "displayName": "潘xn",
      "photoUrl": "",
      "userId": "15665619575579015109"
     },
     "user_tz": -480
    },
    "id": "6851wjGewkJ_"
   },
   "outputs": [],
   "source": [
    "class Multi_layer(nn.Module):\n",
    "    def __init__(self, input_size, n_hiddens, n_class):\n",
    "        \"\"\"\n",
    "        :param input_size:\n",
    "        :param n_hiddens: numbers of hidden layer\n",
    "        :param n_class:\n",
    "        \"\"\"\n",
    "        super(Multi_layer, self).__init__()\n",
    "        layers = OrderedDict()\n",
    "        current_size = input_size\n",
    "        for i, n_hidden in enumerate(n_hiddens):\n",
    "            layers['fc{}'.format(i+1)] = nn.Linear(current_size, n_hidden)\n",
    "            layers['tanh{}'.format(i+1)] = nn.Tanh()\n",
    "            current_size = n_hidden\n",
    "        layers['fc_out'] = nn.Linear(current_size, n_class)\n",
    "        # layers['softmax_out'] = nn.Softmax(dim=1)\n",
    "\n",
    "        self.model = nn.Sequential(layers)\n",
    "#         print(self.model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model.forward(x)\n",
    "        return x\n",
    "\n",
    "def reset_weights(m):\n",
    "    '''\n",
    "        try resetting model weights to avoid weight leakage\n",
    "    '''\n",
    "    for layer in m.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            print(f'Reset trainable parameters of layer = {layer}')\n",
    "            layer.reset_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "executionInfo": {
     "elapsed": 623,
     "status": "ok",
     "timestamp": 1618923009610,
     "user": {
      "displayName": "潘xn",
      "photoUrl": "",
      "userId": "15665619575579015109"
     },
     "user_tz": -480
    },
    "id": "9p4xucUf-SHM"
   },
   "outputs": [],
   "source": [
    "def get_overall_accuracy(model, dataLoader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for _, (pca, target) in enumerate(dataLoader, 0):\n",
    "#         pca, target = pca.cuda(), target.cuda()\n",
    "        outputs = model(pca)\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "def get_class_wise_accuracy(model, dataLoader, n_class=7):\n",
    "    class_correct = [0 for i in range(n_class)]\n",
    "    class_total = [0 for i in range(n_class)]\n",
    "    for (pca, targets) in dataLoader:\n",
    "        outputs = model(pca)\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        c = (predicted == targets).squeeze()\n",
    "\n",
    "        for i in range(c.shape[0]):\n",
    "            target = targets[i]\n",
    "            class_correct[target] += c[i].item()\n",
    "            class_total[target] += 1\n",
    "\n",
    "    return class_correct, class_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the hyper-parameters\n",
    "Also save the parameters of kfold id for the later test and retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1740,
     "status": "ok",
     "timestamp": 1618923013252,
     "user": {
      "displayName": "潘xn",
      "photoUrl": "",
      "userId": "15665619575579015109"
     },
     "user_tz": -480
    },
    "id": "-ISbED8cw07t",
    "outputId": "be6e8b4c-783c-483d-8563-de6fe009733f"
   },
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "batch_size = 16\n",
    "learning_rate = 1e-4\n",
    "epoches = 4000\n",
    "k_fold = 5\n",
    "hidden = [64]\n",
    "dev = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "torch.manual_seed(5)\n",
    "print(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the k-fold cross validator\n",
    "kfold = KFold(n_splits=k_fold, shuffle=True)\n",
    "\n",
    "# save the training ids for the later experiences\n",
    "fold_ids = {}\n",
    "for fold, ids in enumerate(kfold.split(dataset)):\n",
    "    fold_ids[fold] = ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 728,
     "status": "ok",
     "timestamp": 1618923045445,
     "user": {
      "displayName": "潘xn",
      "photoUrl": "",
      "userId": "15665619575579015109"
     },
     "user_tz": -480
    },
    "id": "tSEJMq8sxfyV",
    "outputId": "3a6019e4-5d70-435f-88f1-13a87d2f18a0"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "dataset = csv2dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training script\n",
    "\n",
    "- Use the tensorboard to record the accuracy and loss during the training\n",
    "\n",
    "- save the model after training\n",
    "\n",
    "- different hidden size were trained: [16, 32, 64, 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 537420,
     "status": "ok",
     "timestamp": 1618927335195,
     "user": {
      "displayName": "潘xn",
      "photoUrl": "",
      "userId": "15665619575579015109"
     },
     "user_tz": -480
    },
    "id": "sfD1GGx8xpAJ",
    "outputId": "57493097-7a84-4381-d283-6647197550b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "FOLD 0\n",
      "Sequential(\n",
      "  (fc1): Linear(in_features=10, out_features=128, bias=True)\n",
      "  (tanh1): Tanh()\n",
      "  (fc_out): Linear(in_features=128, out_features=7, bias=True)\n",
      ")\n",
      "Reset trainable parameters of layer = Linear(in_features=10, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=7, bias=True)\n",
      "Start epoch1000\n",
      "Start epoch2000\n",
      "Start epoch3000\n",
      "Start epoch4000\n",
      "Accuracy of 0 : 26 %\n",
      "Accuracy of 1 : 16 %\n",
      "Accuracy of 2 : 40 %\n",
      "Accuracy of 3 : 31 %\n",
      "Accuracy of 4 : 14 %\n",
      "Accuracy of 5 : 33 %\n",
      "Accuracy of 6 : 36 %\n",
      "final accuray28.14814814814815\n",
      "--------------------\n",
      "FOLD 1\n",
      "Sequential(\n",
      "  (fc1): Linear(in_features=10, out_features=128, bias=True)\n",
      "  (tanh1): Tanh()\n",
      "  (fc_out): Linear(in_features=128, out_features=7, bias=True)\n",
      ")\n",
      "Reset trainable parameters of layer = Linear(in_features=10, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=7, bias=True)\n",
      "Start epoch1000\n",
      "Start epoch2000\n",
      "Start epoch3000\n",
      "Start epoch4000\n",
      "Accuracy of 0 : 18 %\n",
      "Accuracy of 1 : 12 %\n",
      "Accuracy of 2 : 36 %\n",
      "Accuracy of 3 : 29 %\n",
      "Accuracy of 4 : 41 %\n",
      "Accuracy of 5 : 27 %\n",
      "Accuracy of 6 : 23 %\n",
      "final accuray27.40740740740741\n",
      "--------------------\n",
      "FOLD 2\n",
      "Sequential(\n",
      "  (fc1): Linear(in_features=10, out_features=128, bias=True)\n",
      "  (tanh1): Tanh()\n",
      "  (fc_out): Linear(in_features=128, out_features=7, bias=True)\n",
      ")\n",
      "Reset trainable parameters of layer = Linear(in_features=10, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=7, bias=True)\n",
      "Start epoch1000\n",
      "Start epoch2000\n",
      "Start epoch3000\n",
      "Start epoch4000\n",
      "Accuracy of 0 : 35 %\n",
      "Accuracy of 1 : 18 %\n",
      "Accuracy of 2 : 31 %\n",
      "Accuracy of 3 : 27 %\n",
      "Accuracy of 4 : 21 %\n",
      "Accuracy of 5 : 18 %\n",
      "Accuracy of 6 :  4 %\n",
      "final accuray22.22222222222222\n",
      "--------------------\n",
      "FOLD 3\n",
      "Sequential(\n",
      "  (fc1): Linear(in_features=10, out_features=128, bias=True)\n",
      "  (tanh1): Tanh()\n",
      "  (fc_out): Linear(in_features=128, out_features=7, bias=True)\n",
      ")\n",
      "Reset trainable parameters of layer = Linear(in_features=10, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=7, bias=True)\n",
      "Start epoch1000\n",
      "Start epoch2000\n",
      "Start epoch3000\n",
      "Start epoch4000\n",
      "Accuracy of 0 : 30 %\n",
      "Accuracy of 1 :  5 %\n",
      "Accuracy of 2 : 14 %\n",
      "Accuracy of 3 : 45 %\n",
      "Accuracy of 4 : 21 %\n",
      "Accuracy of 5 : 27 %\n",
      "Accuracy of 6 :  7 %\n",
      "final accuray22.962962962962962\n",
      "--------------------\n",
      "FOLD 4\n",
      "Sequential(\n",
      "  (fc1): Linear(in_features=10, out_features=128, bias=True)\n",
      "  (tanh1): Tanh()\n",
      "  (fc_out): Linear(in_features=128, out_features=7, bias=True)\n",
      ")\n",
      "Reset trainable parameters of layer = Linear(in_features=10, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=7, bias=True)\n",
      "Start epoch1000\n",
      "Start epoch2000\n",
      "Start epoch3000\n",
      "Start epoch4000\n",
      "Accuracy of 0 : 22 %\n",
      "Accuracy of 1 : 23 %\n",
      "Accuracy of 2 : 29 %\n",
      "Accuracy of 3 : 44 %\n",
      "Accuracy of 4 : 15 %\n",
      "Accuracy of 5 : 40 %\n",
      "Accuracy of 6 : 19 %\n",
      "final accuray28.35820895522388\n",
      "training finished\n"
     ]
    }
   ],
   "source": [
    "s = '1_SFEW_h128_b16_lr4_ep4000'\n",
    "writer = SummaryWriter('runs/' + s)\n",
    "\n",
    "# define list to store results through training process\n",
    "train_losses = []\n",
    "\n",
    "# define class-wise AC dict, record cw_ac for each fold, and \n",
    "cw_ac_dict = {}\n",
    "\n",
    "# K-fold cross validation model evaluation\n",
    "for fold, (train_ids, test_ids) in fold_ids.items():\n",
    "    print('--------------------')\n",
    "    print(f'FOLD {fold}')\n",
    "\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size,\n",
    "        sampler = train_subsampler\n",
    "        )\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size,\n",
    "        sampler = test_subsampler\n",
    "        )\n",
    "\n",
    "    # initial model\n",
    "    model = Multi_layer(10, [128], 7).to(dev)\n",
    "    model.apply(reset_weights)\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "    current_loss = 0.\n",
    "    test_loss = 0.\n",
    "    for epoch in range(epoches):\n",
    "        if epoch%1000 == 999:\n",
    "            print(f'Start epoch{epoch+1}')\n",
    "\n",
    "        for i, data in enumerate(trainloader, start=0):\n",
    "            inputs, targets = data\n",
    "#             inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        current_loss += loss.item()\n",
    "        \n",
    "        #compute test loss\n",
    "        with torch.no_grad():\n",
    "            #randomly select only one batch\n",
    "            inputs, targets = iter(testloader).next()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item()\n",
    "                \n",
    "\n",
    "        if epoch%100 == 99:\n",
    "            train_ac = get_overall_accuracy(model, trainloader)\n",
    "            test_ac = get_overall_accuracy(model, testloader)\n",
    "\n",
    "            writer.add_scalar(f'fold {fold} loss/training loss', current_loss/100, epoch)\n",
    "            writer.add_scalar(f'fold {fold} loss/test loss', test_loss/100, epoch)\n",
    "            writer.add_scalar(f'fold {fold} AC/training accuracy', train_ac, epoch)\n",
    "            writer.add_scalar(f'fold {fold} AC/test accuracy', test_ac, epoch)\n",
    "            \n",
    "            current_loss = 0.\n",
    "            test_loss = 0.\n",
    "    \n",
    "    # calculate the cw_ac and save in dict\n",
    "    correct, total = get_class_wise_accuracy(model, testloader)\n",
    "    m_accuracy = [c/t for (c,t) in zip(correct, total)]\n",
    "    ac_dict = {}    \n",
    "    for i in range(len(m_accuracy)):\n",
    "        print('Accuracy of %d : %2d %%' %\n",
    "            (i, m_accuracy[i]*100))\n",
    "        ac_dict[i] = m_accuracy[i]*100  \n",
    "    final_ac = get_overall_accuracy(model, testloader)\n",
    "    print(f'final accuray{final_ac}')\n",
    "    \n",
    "    ac_dict['average'] = final_ac\n",
    "    cw_ac_dict[fold] = ac_dict\n",
    "    \n",
    "    save_path = 'models/'+ s + f'_model-fold{fold}.pth'\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "writer.close()\n",
    "print('training finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning the network\n",
    "After training the network, I need to use different technique to pruning the network\n",
    "- the distictiveness similarity\n",
    "- the distictiveness non-functional units\n",
    "\n",
    "The distinctiveness compare the angle between the output of units and also find the non-functional units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# name of path that the model is saved, along with the hidden unit size\n",
    "path_set = {\n",
    "    '2_SFEW_h16_b16_lr4_ep6000': [16],\n",
    "    '1_SFEW_h32_b16_lr4_ep6000': [32],\n",
    "    '4_SFEW_h64_b16_lr4_ep6000': [64],\n",
    "    '2_SFEW_h128_b16_lr4_ep6000': [128],\n",
    "    \n",
    "    '1_SFEW_h16_b16_lr4_ep4000': [16],\n",
    "    '1_SFEW_h32_b16_lr4_ep4000': [32],\n",
    "    '2_SFEW_h64_b16_lr4_ep4000': [64],    \n",
    "    '1_SFEW_h128_b16_lr4_ep4000': [128]\n",
    "}\n",
    "type(path_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the second fold as the final test ac is most similar\n",
    "ids = fold_ids[1]\n",
    "train_ids, test_ids = ids #len(test_ids) = 135\n",
    "test_patterns, test_targets = dataset[test_ids]\n",
    "train_patterns, train_targets = dataset[train_ids]\n",
    "\n",
    "train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "                            dataset, batch_size=batch_size,\n",
    "                            sampler = train_subsampler)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "                            dataset, batch_size=batch_size,\n",
    "                            sampler = test_subsampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data to save\n",
    "first_layer_output = {}\n",
    "units_angles = {}\n",
    "units_output_mean = {}\n",
    "units_output_std = {}\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Define a pruning function:\n",
    "    - Prune units the list of index of units that need to remove\n",
    "    - similar units: a dict[unit_to_keep] = list of other units that are similar to unit_to_keep\n",
    "\"\"\"\n",
    "def pruning_network(model_old, prune_unites, similar_units=None):\n",
    "    \n",
    "    fc = model_old.model[0].weight.clone().detach()\n",
    "    fc_b = model_old.model[0].bias.clone().detach()\n",
    "    fc_out = model_old.model[2].weight\n",
    "    fc_out_b = model_old.model[2].bias\n",
    "    \n",
    "    if similar_units != None:\n",
    "        for idx, unit_list in similar_units.items():\n",
    "            temp = fc[idx]\n",
    "            temp_b = fc_b[idx]\n",
    "            for jdx in unit_list:\n",
    "                temp += fc[jdx]\n",
    "                temp_b = fc_b[jdx]\n",
    "            temp /= len(unit_list) + 1\n",
    "            temp_b /= len(unit_list) + 1\n",
    "            \n",
    "            fc[idx] = temp\n",
    "            fc_b[idx] = temp_b\n",
    "            \n",
    "    h_units = fc.shape[0]\n",
    "    msk = set(range(h_units)) - prune_unites\n",
    "    msk = list(msk)\n",
    "    \n",
    "    model_new = Multi_layer(10, [len(msk)], 7)\n",
    "    model_new.model[0].weight = nn.Parameter(fc[msk])\n",
    "    model_new.model[0].bias = nn.Parameter(fc_b[msk])\n",
    "    model_new.model[2].weight = nn.Parameter(fc_out[:, msk])\n",
    "    model_new.model[2].bias = nn.Parameter(fc_out_b)\n",
    "    \n",
    "    return model_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2_SFEW_h16_b16_lr4_ep6000\n",
      "number of pairs: 120\n",
      "angle range: [57.15, 124.40]\n",
      "mean range: [-0.5884, 0.4066]\n",
      "std range: [0.5085, 0.7827]\n",
      "\n",
      "\n",
      "1_SFEW_h32_b16_lr4_ep6000\n",
      "number of pairs: 496\n",
      "angle range: [27.13, 135.62]\n",
      "mean range: [-0.5758, 0.3765]\n",
      "std range: [0.4555, 0.7930]\n",
      "\n",
      "\n",
      "4_SFEW_h64_b16_lr4_ep6000\n",
      "number of pairs: 2016\n",
      "angle range: [23.77, 163.18]\n",
      "mean range: [-0.5186, 0.4368]\n",
      "std range: [0.3599, 0.7915]\n",
      "\n",
      "\n",
      "2_SFEW_h128_b16_lr4_ep6000\n",
      "number of pairs: 8128\n",
      "angle range: [31.51, 156.83]\n",
      "mean range: [-0.4671, 0.4229]\n",
      "std range: [0.2912, 0.7940]\n",
      "\n",
      "\n",
      "1_SFEW_h16_b16_lr4_ep4000\n",
      "number of pairs: 120\n",
      "angle range: [31.08, 125.56]\n",
      "mean range: [-0.3090, 0.2955]\n",
      "std range: [0.5202, 0.7498]\n",
      "\n",
      "\n",
      "1_SFEW_h32_b16_lr4_ep4000\n",
      "number of pairs: 496\n",
      "angle range: [36.75, 138.31]\n",
      "mean range: [-0.3866, 0.2351]\n",
      "std range: [0.4315, 0.7584]\n",
      "\n",
      "\n",
      "2_SFEW_h64_b16_lr4_ep4000\n",
      "number of pairs: 2016\n",
      "angle range: [19.39, 146.00]\n",
      "mean range: [-0.4573, 0.3139]\n",
      "std range: [0.3382, 0.7601]\n",
      "\n",
      "\n",
      "1_SFEW_h128_b16_lr4_ep4000\n",
      "number of pairs: 8128\n",
      "angle range: [22.16, 156.49]\n",
      "mean range: [-0.3357, 0.4267]\n",
      "std range: [0.2600, 0.7197]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate the angle between units in different model, and print it\n",
    "for path in path_set:\n",
    "    # load model\n",
    "    print(path)\n",
    "    load_path = 'models/'+ path + '_model-fold1.pth'\n",
    "    model = Multi_layer(10, path_set[path], 7)\n",
    "    model.load_state_dict(torch.load(load_path))\n",
    "    models[path] = model\n",
    "    \n",
    "    # calculate the patterns outputs\n",
    "    fc = model.model[0]\n",
    "    tanh = model.model[1]\n",
    "    output = tanh(fc(train_patterns))\n",
    "    \n",
    "    first_layer_output[path] = output\n",
    "    \n",
    "    angles = {}\n",
    "    temp = output.clone().detach().T\n",
    "    for i in range(temp.shape[0]):\n",
    "        for j in range(i+1, temp.shape[0]):\n",
    "            a = temp[i]\n",
    "            b = temp[j]\n",
    "            inner = torch.inner(a, b)\n",
    "            a_norm = a.pow(2).sum().pow(0.5)\n",
    "            b_norm = b.pow(2).sum().pow(0.5)\n",
    "            cos = inner / (a_norm * b_norm)\n",
    "            angle = torch.acos(cos) * (180/math.pi)\n",
    "            angles[(i, j)] = angle\n",
    "    \n",
    "    # save angles in dict\n",
    "    units_angles[path] = angles\n",
    "    print(f'number of pairs: {len(angles)}')\n",
    "    \n",
    "    # print range of angles\n",
    "    min_a = 180\n",
    "    max_a = 0 \n",
    "    for s, angle in angles.items():\n",
    "        if angle.item() < min_a:\n",
    "            min_a = angle.item()\n",
    "        if angle.item() > max_a:\n",
    "            max_a = angle.item()\n",
    "    print('angle range: [%.2f, %.2f]'%(min_a, max_a))\n",
    "    \n",
    "    # To find the non-functional units, calculate the mean and std of the output\n",
    "    mean = output.sum(axis=0)/(output.shape[0])\n",
    "    std = output.std(axis=0)\n",
    "    units_output_mean[path] = mean\n",
    "    units_output_std[path] = std\n",
    "    print('mean range: [%.4f, %.4f]' % (min(mean), max(mean)))\n",
    "    print('std range: [%.4f, %.4f]' % (min(std), max(std)))\n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2_SFEW_h16_b16_lr4_ep6000\n",
      "set size: 1\n",
      "\n",
      "1_SFEW_h32_b16_lr4_ep6000\n",
      "set size: 4\n",
      "\n",
      "4_SFEW_h64_b16_lr4_ep6000\n",
      "set size: 5\n",
      "\n",
      "2_SFEW_h128_b16_lr4_ep6000\n",
      "set size: 24\n",
      "\n",
      "1_SFEW_h16_b16_lr4_ep4000\n",
      "set size: 3\n",
      "\n",
      "1_SFEW_h32_b16_lr4_ep4000\n",
      "set size: 7\n",
      "\n",
      "2_SFEW_h64_b16_lr4_ep4000\n",
      "set size: 15\n",
      "\n",
      "1_SFEW_h128_b16_lr4_ep4000\n",
      "set size: 20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To find the units that bounds in -1, 0, 1 -> [-1, -0.9], [-0.05, 0.05], [0.95, 1]\n",
    "bound_units = {}\n",
    "for path in path_set:\n",
    "    print(path)\n",
    "    bound_set = set()\n",
    "    mean = units_output_mean[path]\n",
    "    std = units_output_std[path]\n",
    "    for i, value in enumerate(mean):\n",
    "        if value<-0.9 or (-0.05<value<0.05) or value>0.9:\n",
    "#             print('%d: mean: %.2f, std: %.2f' % (i, mean[i], std[i]))\n",
    "            bound_set.add(i)\n",
    "    bound_units[path] = bound_set\n",
    "    print(f'set size: {len(bound_set)}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2_SFEW_h16_b16_lr4_ep6000\n",
      "prune units: 1\n",
      "1.92\n",
      "2.00\n",
      "27.41\n",
      "28.15\n",
      "\n",
      "\n",
      "\n",
      "1_SFEW_h32_b16_lr4_ep6000\n",
      "prune units: 4\n",
      "1.97\n",
      "2.01\n",
      "28.15\n",
      "28.15\n",
      "\n",
      "\n",
      "\n",
      "4_SFEW_h64_b16_lr4_ep6000\n",
      "prune units: 5\n",
      "1.95\n",
      "2.16\n",
      "28.89\n",
      "25.93\n",
      "\n",
      "\n",
      "\n",
      "2_SFEW_h128_b16_lr4_ep6000\n",
      "prune units: 24\n",
      "2.03\n",
      "2.32\n",
      "30.37\n",
      "27.41\n",
      "\n",
      "\n",
      "\n",
      "1_SFEW_h16_b16_lr4_ep4000\n",
      "prune units: 3\n",
      "1.89\n",
      "1.96\n",
      "28.89\n",
      "21.48\n",
      "\n",
      "\n",
      "\n",
      "1_SFEW_h32_b16_lr4_ep4000\n",
      "prune units: 7\n",
      "1.83\n",
      "1.90\n",
      "28.15\n",
      "23.70\n",
      "\n",
      "\n",
      "\n",
      "2_SFEW_h64_b16_lr4_ep4000\n",
      "prune units: 15\n",
      "1.90\n",
      "1.99\n",
      "30.37\n",
      "29.63\n",
      "\n",
      "\n",
      "\n",
      "1_SFEW_h128_b16_lr4_ep4000\n",
      "prune units: 20\n",
      "1.91\n",
      "1.98\n",
      "27.41\n",
      "25.19\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pruning the network of bound units\n",
    "models_new = {}\n",
    "\n",
    "for path in path_set:\n",
    "    print(path)\n",
    "    \n",
    "    model_old = models[path]\n",
    "    prune_units = bound_units[path]\n",
    "    print(f'prune units: {len(prune_units)}')\n",
    "    \n",
    "    if len(prune_units) == 0:\n",
    "        model_new = model_old\n",
    "    else:\n",
    "        model_new = pruning_network(model_old, prune_units)\n",
    "    \n",
    "    models_new[path] = model_new\n",
    "    \n",
    "    outputs = model_old(test_patterns)\n",
    "    loss = criterion(outputs, test_targets).item()\n",
    "    print('%.2f' % loss)\n",
    "    \n",
    "    outputs = model_new(patterns)\n",
    "    loss = criterion(outputs, test_targets).item()\n",
    "    print('%.2f' % loss)\n",
    "    \n",
    "    print('%.2f' % get_overall_accuracy(model_old, testloader))\n",
    "    print('%.2f' % get_overall_accuracy(model_new, testloader))\n",
    "    print('\\n')\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2_SFEW_h16_b16_lr4_ep6000\n",
      "retrained loss: 1.95\n",
      "retrain ac: 32.59\n",
      "\n",
      "1_SFEW_h32_b16_lr4_ep6000\n",
      "retrained loss: 1.98\n",
      "retrain ac: 28.89\n",
      "\n",
      "4_SFEW_h64_b16_lr4_ep6000\n",
      "retrained loss: 1.96\n",
      "retrain ac: 26.67\n",
      "\n",
      "2_SFEW_h128_b16_lr4_ep6000\n",
      "retrained loss: 1.98\n",
      "retrain ac: 34.07\n",
      "\n",
      "1_SFEW_h16_b16_lr4_ep4000\n",
      "retrained loss: 1.90\n",
      "retrain ac: 25.93\n",
      "\n",
      "1_SFEW_h32_b16_lr4_ep4000\n",
      "retrained loss: 1.84\n",
      "retrain ac: 29.63\n",
      "\n",
      "2_SFEW_h64_b16_lr4_ep4000\n",
      "retrained loss: 1.91\n",
      "retrain ac: 30.37\n",
      "\n",
      "1_SFEW_h128_b16_lr4_ep4000\n",
      "retrained loss: 1.93\n",
      "retrain ac: 27.41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## retrain the pruned model for 500 epoches\n",
    "retrained_epoches = 500\n",
    "model_bound_retrain = {}\n",
    "\n",
    "for path in path_set:\n",
    "    print(path)\n",
    "    model = models_new[path]\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    \n",
    "    for epoches in range(retrained_epoches):\n",
    "        for i, (inputs, targets) in enumerate(trainloader, 0):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    model_bound_retrain[path] = model\n",
    "    \n",
    "    outputs = model(test_patterns)\n",
    "    loss = criterion(outputs, test_targets).item()\n",
    "    print('retrained loss: %.2f' % loss)\n",
    "    \n",
    "    ac = get_overall_accuracy(model, testloader)\n",
    "    print('retrain ac: %.2f' % ac)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find similar angles and complementary ones\n",
    "similar_units = {}\n",
    "complementary_units = {}\n",
    "for path in path_set:\n",
    "    similar = []\n",
    "    complementary = set()\n",
    "    angles = units_angles[path]\n",
    "    for pair, value in angles.items():\n",
    "        if value<30:\n",
    "            similar.append(pair)\n",
    "        if value>150:\n",
    "            a, b = pair\n",
    "            complementary.add(a)\n",
    "            complementary.add(b)\n",
    "    \n",
    "    similar_units[path] = similar\n",
    "    complementary_units[path] = complementary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2_SFEW_h16_b16_lr4_ep6000': {},\n",
       " '1_SFEW_h32_b16_lr4_ep6000': {29: {30}},\n",
       " '4_SFEW_h64_b16_lr4_ep6000': {33: {57}},\n",
       " '2_SFEW_h128_b16_lr4_ep6000': {},\n",
       " '1_SFEW_h16_b16_lr4_ep4000': {},\n",
       " '1_SFEW_h32_b16_lr4_ep4000': {},\n",
       " '2_SFEW_h64_b16_lr4_ep4000': {2: {40}, 30: {50}},\n",
       " '1_SFEW_h128_b16_lr4_ep4000': {6: {35}, 15: {47}, 16: {33, 104}}}"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for path in path_set:\n",
    "    c = similar_units[path]\n",
    "    if len(c) ==0: \n",
    "        similar_units[path] = {}\n",
    "        continue\n",
    "    i2j = dict()\n",
    "    j2i = dict()\n",
    "\n",
    "    # merge similar group\n",
    "    for i, j in c:\n",
    "        if i not in i2j.keys():\n",
    "            if j not in j2i.keys():\n",
    "                i2j[i] = {j}\n",
    "                j2i[j] = i\n",
    "            else:\n",
    "                i2j[j2i[j]].add(j)\n",
    "        \n",
    "                    \n",
    "        elif i in i2j.keys():\n",
    "            i2j[i].add(j)\n",
    "            j2i[j] = i\n",
    "    similar_units[path] = i2j\n",
    "    for s in i2j.values():\n",
    "        for value in s:\n",
    "            complementary_units[path].add(value)\n",
    "similar_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2_SFEW_h16_b16_lr4_ep6000\n",
      "0\n",
      "0\n",
      "26.67\n",
      "26.67\n",
      "28.15\n",
      "1.96\n",
      "1.96\n",
      "1.98\n",
      "\n",
      "\n",
      "1_SFEW_h32_b16_lr4_ep6000\n",
      "1\n",
      "1\n",
      "27.41\n",
      "30.37\n",
      "28.89\n",
      "1.99\n",
      "2.00\n",
      "2.01\n",
      "\n",
      "\n",
      "4_SFEW_h64_b16_lr4_ep6000\n",
      "1\n",
      "3\n",
      "28.89\n",
      "25.19\n",
      "28.15\n",
      "1.96\n",
      "2.07\n",
      "1.98\n",
      "\n",
      "\n",
      "2_SFEW_h128_b16_lr4_ep6000\n",
      "0\n",
      "10\n",
      "30.37\n",
      "21.48\n",
      "28.89\n",
      "2.03\n",
      "2.20\n",
      "2.05\n",
      "\n",
      "\n",
      "1_SFEW_h16_b16_lr4_ep4000\n",
      "0\n",
      "0\n",
      "28.89\n",
      "28.89\n",
      "31.85\n",
      "1.91\n",
      "1.91\n",
      "1.93\n",
      "\n",
      "\n",
      "1_SFEW_h32_b16_lr4_ep4000\n",
      "0\n",
      "0\n",
      "27.41\n",
      "27.41\n",
      "27.41\n",
      "1.85\n",
      "1.85\n",
      "1.87\n",
      "\n",
      "\n",
      "2_SFEW_h64_b16_lr4_ep4000\n",
      "2\n",
      "2\n",
      "28.15\n",
      "28.89\n",
      "26.67\n",
      "1.90\n",
      "1.92\n",
      "1.92\n",
      "\n",
      "\n",
      "1_SFEW_h128_b16_lr4_ep4000\n",
      "3\n",
      "18\n",
      "26.67\n",
      "25.19\n",
      "24.44\n",
      "1.91\n",
      "2.07\n",
      "1.93\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prune network by angles\n",
    "models_angle = {}\n",
    "models_angle_retrain = {}\n",
    "for path in path_set:\n",
    "    print(path)\n",
    "    similar = similar_units[path]\n",
    "    remove = complementary_units[path]\n",
    "    s = len(similar.keys())\n",
    "    r = len(remove)\n",
    "    print(f'{s}')\n",
    "    print(f'{r}')\n",
    "    \n",
    "    model_old = models[path]\n",
    "    if s == 0 and r == 0:\n",
    "        model_new = model_old\n",
    "    else:\n",
    "        model_new = pruning_network(model_old, remove, similar)\n",
    "    \n",
    "    models_angle[path] = model_new\n",
    "    outputs = model_old(test_patterns)\n",
    "    loss_o = criterion(outputs, test_targets).item()\n",
    "    \n",
    "    \n",
    "    outputs = model_new(patterns)\n",
    "    loss_n = criterion(outputs, test_targets).item()\n",
    "    \n",
    "    \n",
    "    ac_o = get_overall_accuracy(model_old, testloader)\n",
    "    ac_n = get_overall_accuracy(model_new, testloader)\n",
    "    \n",
    "    optimizer = optim.SGD(model_new.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    \n",
    "    for epoches in range(retrained_epoches):\n",
    "        for i, (inputs, targets) in enumerate(trainloader, 0):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model_new(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    models_angle_retrain[path] = model_new\n",
    "    \n",
    "    outputs = model_new(test_patterns)\n",
    "    loss_r = criterion(outputs, test_targets).item()\n",
    "    \n",
    "    \n",
    "    ac = get_overall_accuracy(model_new, testloader)\n",
    "    print('%.2f' % ac_o)\n",
    "    print('%.2f' % ac_n)\n",
    "    print('%.2f' % ac)\n",
    "    \n",
    "    print('%.2f' % loss_o)\n",
    "    print('%.2f' % loss_n)\n",
    "    print('%.2f' % loss_r)\n",
    "    \n",
    "    print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
